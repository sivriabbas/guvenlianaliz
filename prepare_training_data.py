"""
ğŸ“Š DATASET HAZIRLAMA - PHASE 7.B1
==================================
training_dataset.csv'yi ML model eÄŸitimi iÃ§in hazÄ±rla
Train/Test split, normalization, feature engineering
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils import class_weight
import joblib
import json
from datetime import datetime


class DatasetPreparator:
    """Training dataset'i hazÄ±rla"""
    
    def __init__(self, csv_path='training_dataset.csv'):
        self.csv_path = csv_path
        self.df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        
        print(f"âœ… Dataset Preparator baÅŸlatÄ±ldÄ±")
    
    def load_data(self):
        """CSV'den veriyi yÃ¼kle"""
        print(f"\nğŸ“‚ Veri yÃ¼kleniyor: {self.csv_path}")
        
        try:
            self.df = pd.read_csv(self.csv_path)
            print(f"âœ… {len(self.df)} satÄ±r, {len(self.df.columns)} sÃ¼tun yÃ¼klendi")
            return True
        except FileNotFoundError:
            print(f"âŒ Dosya bulunamadÄ±: {self.csv_path}")
            print("âš ï¸  Ã–nce calculate_historical_factors.py Ã§alÄ±ÅŸtÄ±rÄ±n!")
            return False
        except Exception as e:
            print(f"âŒ YÃ¼kleme hatasÄ±: {str(e)}")
            return False
    
    def explore_data(self):
        """Veri keÅŸfi"""
        print("\n" + "="*80)
        print("ğŸ” VERÄ° KEÅFÄ°")
        print("="*80)
        
        # Temel bilgiler
        print(f"\nğŸ“Š Dataset Boyutu:")
        print(f"   SatÄ±rlar: {len(self.df)}")
        print(f"   SÃ¼tunlar: {len(self.df.columns)}")
        
        # Ä°lk 5 satÄ±r
        print(f"\nğŸ“‹ Ä°lk 5 SatÄ±r:")
        print(self.df.head().to_string())
        
        # Veri tipleri
        print(f"\nğŸ”¢ Veri Tipleri:")
        print(self.df.dtypes.value_counts())
        
        # Eksik deÄŸerler
        missing = self.df.isnull().sum()
        if missing.sum() > 0:
            print(f"\nâš ï¸  Eksik DeÄŸerler:")
            print(missing[missing > 0])
        else:
            print(f"\nâœ… Eksik deÄŸer yok")
        
        # Hedef deÄŸiÅŸken daÄŸÄ±lÄ±mÄ±
        if 'result' in self.df.columns:
            print(f"\nğŸ¯ Hedef DeÄŸiÅŸken DaÄŸÄ±lÄ±mÄ± (result):")
            result_counts = self.df['result'].value_counts()
            for result, count in result_counts.items():
                label = {'H': 'Ev Sahibi Galibiyet', 'A': 'Deplasman Galibiyet', 'D': 'Beraberlik'}[result]
                pct = (count / len(self.df)) * 100
                print(f"   {label:25}: {count:4} ({pct:5.1f}%)")
        
        # Temel istatistikler
        print(f"\nğŸ“ˆ Temel Ä°statistikler:")
        print(self.df.describe().to_string())
    
    def feature_engineering(self):
        """Ã–zellik mÃ¼hendisliÄŸi"""
        print("\n" + "="*80)
        print("âš™ï¸ Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ°")
        print("="*80)
        
        initial_features = len(self.df.columns)
        
        # 1. ELO tabanlÄ± Ã¶zellikler
        if 'home_elo' in self.df.columns and 'away_elo' in self.df.columns:
            self.df['elo_ratio'] = self.df['home_elo'] / (self.df['away_elo'] + 1)
            self.df['elo_sum'] = self.df['home_elo'] + self.df['away_elo']
            print("   âœ… ELO ratio ve sum eklendi")
        
        # 2. Form tabanlÄ± Ã¶zellikler
        if 'home_form' in self.df.columns and 'away_form' in self.df.columns:
            self.df['form_ratio'] = self.df['home_form'] / (self.df['away_form'] + 1)
            self.df['form_sum'] = self.df['home_form'] + self.df['away_form']
            print("   âœ… Form ratio ve sum eklendi")
        
        # 3. Pozisyon tabanlÄ± Ã¶zellikler
        if 'home_position' in self.df.columns and 'away_position' in self.df.columns:
            self.df['position_sum'] = self.df['home_position'] + self.df['away_position']
            self.df['top_clash'] = ((self.df['home_position'] <= 5) & 
                                   (self.df['away_position'] <= 5)).astype(int)
            self.df['relegation_battle'] = ((self.df['home_position'] >= 15) | 
                                           (self.df['away_position'] >= 15)).astype(int)
            print("   âœ… Pozisyon Ã¶zellikler eklendi")
        
        # 4. H2H tabanlÄ± Ã¶zellikler
        if 'h2h_win_rate' in self.df.columns:
            self.df['h2h_advantage'] = (self.df['h2h_win_rate'] - 0.5) * 2  # -1 ile 1 arasÄ±
            print("   âœ… H2H advantage eklendi")
        
        # 5. Ev avantajÄ± composite
        if 'home_advantage' in self.df.columns and 'away_disadvantage' in self.df.columns:
            self.df['total_home_edge'] = self.df['home_advantage'] + self.df['away_disadvantage']
            print("   âœ… Total home edge eklendi")
        
        # 6. Lig bazlÄ± encoding
        if 'league' in self.df.columns:
            league_dummies = pd.get_dummies(self.df['league'], prefix='league')
            self.df = pd.concat([self.df, league_dummies], axis=1)
            print(f"   âœ… Lig one-hot encoding ({len(league_dummies.columns)} Ã¶zellik)")
        
        # 7. Sezon bazlÄ± encoding
        if 'season' in self.df.columns:
            season_dummies = pd.get_dummies(self.df['season'], prefix='season')
            self.df = pd.concat([self.df, season_dummies], axis=1)
            print(f"   âœ… Sezon one-hot encoding ({len(season_dummies.columns)} Ã¶zellik)")
        
        final_features = len(self.df.columns)
        added = final_features - initial_features
        
        print(f"\n   ğŸ“Š {initial_features} â†’ {final_features} Ã¶zellik (+{added})")
    
    def prepare_features(self):
        """Model iÃ§in Ã¶zellik ve hedef deÄŸiÅŸkenleri hazÄ±rla"""
        print("\n" + "="*80)
        print("ğŸ¯ Ã–ZELLÄ°K HAZIRLAMA")
        print("="*80)
        
        # Meta bilgileri kaldÄ±r
        meta_columns = ['match_id', 'date', 'home_team', 'away_team', 'league', 'season',
                       'home_goals', 'away_goals', 'result']
        
        # Ã–zellik sÃ¼tunlarÄ±
        feature_columns = [col for col in self.df.columns if col not in meta_columns]
        
        print(f"\n   ğŸ“‹ Toplam {len(feature_columns)} Ã¶zellik kullanÄ±lacak:")
        for i, col in enumerate(feature_columns, 1):
            if i <= 10 or i > len(feature_columns) - 5:
                print(f"      {i:2}. {col}")
            elif i == 11:
                print(f"      ... ({len(feature_columns) - 15} Ã¶zellik daha)")
        
        # X (features) ve y (target)
        X = self.df[feature_columns]
        
        # Hedef deÄŸiÅŸkeni encode et: H=2, D=1, A=0
        y = self.df['result'].map({'H': 2, 'D': 1, 'A': 0})
        
        print(f"\n   âœ… X shape: {X.shape}")
        print(f"   âœ… y shape: {y.shape}")
        
        # Eksik deÄŸerleri doldur
        if X.isnull().sum().sum() > 0:
            print(f"\n   âš ï¸  Eksik deÄŸerler dolduruluyor...")
            X = X.fillna(X.mean())
        
        # Sonsuz deÄŸerleri kontrol et
        X = X.replace([np.inf, -np.inf], 0)
        
        return X, y, feature_columns
    
    def split_data(self, X, y, test_size=0.2, random_state=42):
        """Train/Test split"""
        print("\n" + "="*80)
        print("âœ‚ï¸ TRAIN/TEST SPLIT")
        print("="*80)
        
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        print(f"\n   ğŸ“Š Train Set: {len(self.X_train)} samples ({(1-test_size)*100:.0f}%)")
        print(f"   ğŸ“Š Test Set:  {len(self.X_test)} samples ({test_size*100:.0f}%)")
        
        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
        print(f"\n   ğŸ¯ Train Set DaÄŸÄ±lÄ±mÄ±:")
        train_dist = self.y_train.value_counts().sort_index()
        for class_id, count in train_dist.items():
            label = {2: 'Ev Galibiyet', 1: 'Beraberlik', 0: 'Deplasman Galibiyet'}[class_id]
            pct = (count / len(self.y_train)) * 100
            print(f"      {label:20}: {count:4} ({pct:5.1f}%)")
        
        print(f"\n   ğŸ¯ Test Set DaÄŸÄ±lÄ±mÄ±:")
        test_dist = self.y_test.value_counts().sort_index()
        for class_id, count in test_dist.items():
            label = {2: 'Ev Galibiyet', 1: 'Beraberlik', 0: 'Deplasman Galibiyet'}[class_id]
            pct = (count / len(self.y_test)) * 100
            print(f"      {label:20}: {count:4} ({pct:5.1f}%)")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def normalize_data(self):
        """Veriyi normalize et (StandardScaler)"""
        print("\n" + "="*80)
        print("ğŸ“ NORMALÄ°ZASYON")
        print("="*80)
        
        # Train set Ã¼zerinde fit et
        self.scaler.fit(self.X_train)
        
        # Her iki seti de transform et
        self.X_train_scaled = self.scaler.transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        # DataFrame'e Ã§evir
        self.X_train_scaled = pd.DataFrame(
            self.X_train_scaled, 
            columns=self.X_train.columns,
            index=self.X_train.index
        )
        self.X_test_scaled = pd.DataFrame(
            self.X_test_scaled,
            columns=self.X_test.columns,
            index=self.X_test.index
        )
        
        print(f"\n   âœ… Train set normalize edildi: {self.X_train_scaled.shape}")
        print(f"   âœ… Test set normalize edildi: {self.X_test_scaled.shape}")
        
        # Ã–rnek istatistikler
        print(f"\n   ğŸ“Š Normalize EdilmiÅŸ Veri Ã–zeti (ilk 5 Ã¶zellik):")
        print(self.X_train_scaled.iloc[:, :5].describe().to_string())
    
    def calculate_class_weights(self):
        """Dengesiz sÄ±nÄ±flar iÃ§in aÄŸÄ±rlÄ±klar hesapla"""
        print("\n" + "="*80)
        print("âš–ï¸ SINIF AÄIRLIKLARI")
        print("="*80)
        
        # Sklearn class_weight hesapla
        classes = np.unique(self.y_train)
        weights = class_weight.compute_class_weight(
            'balanced',
            classes=classes,
            y=self.y_train
        )
        
        class_weights_dict = dict(zip(classes, weights))
        
        print(f"\n   ğŸ“Š Hesaplanan AÄŸÄ±rlÄ±klar:")
        for class_id, weight in class_weights_dict.items():
            label = {2: 'Ev Galibiyet', 1: 'Beraberlik', 0: 'Deplasman Galibiyet'}[class_id]
            print(f"      {label:20}: {weight:.3f}")
        
        return class_weights_dict
    
    def save_datasets(self, output_dir='prepared_data'):
        """HazÄ±rlanan datasetleri kaydet"""
        print("\n" + "="*80)
        print("ğŸ’¾ KAYDETME")
        print("="*80)
        
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # NumPy arrays olarak kaydet (hÄ±zlÄ± yÃ¼kleme iÃ§in)
        np.save(f'{output_dir}/X_train.npy', self.X_train_scaled.values)
        np.save(f'{output_dir}/X_test.npy', self.X_test_scaled.values)
        np.save(f'{output_dir}/y_train.npy', self.y_train.values)
        np.save(f'{output_dir}/y_test.npy', self.y_test.values)
        
        # Scaler'Ä± kaydet
        joblib.dump(self.scaler, f'{output_dir}/scaler.pkl')
        
        # Feature names kaydet
        feature_names = self.X_train.columns.tolist()
        with open(f'{output_dir}/feature_names.json', 'w') as f:
            json.dump(feature_names, f, indent=2)
        
        # Metadata kaydet
        metadata = {
            'created_at': datetime.now().isoformat(),
            'total_samples': len(self.df),
            'train_samples': len(self.X_train),
            'test_samples': len(self.X_test),
            'n_features': len(feature_names),
            'test_size': 0.2,
            'random_state': 42,
            'class_distribution': {
                'train': self.y_train.value_counts().to_dict(),
                'test': self.y_test.value_counts().to_dict()
            }
        }
        
        with open(f'{output_dir}/metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"\n   âœ… X_train.npy ({self.X_train_scaled.shape})")
        print(f"   âœ… X_test.npy ({self.X_test_scaled.shape})")
        print(f"   âœ… y_train.npy ({self.y_train.shape})")
        print(f"   âœ… y_test.npy ({self.y_test.shape})")
        print(f"   âœ… scaler.pkl")
        print(f"   âœ… feature_names.json ({len(feature_names)} features)")
        print(f"   âœ… metadata.json")
        
        print(f"\n   ğŸ“ KlasÃ¶r: {output_dir}/")
    
    def process(self):
        """Tam iÅŸlem pipeline'Ä±"""
        print("\n" + "="*80)
        print("ğŸš€ DATASET HAZIRLAMA BAÅLADI")
        print("="*80)
        
        # 1. Veri yÃ¼kle
        if not self.load_data():
            return False
        
        # 2. Veri keÅŸfi
        self.explore_data()
        
        # 3. Feature engineering
        self.feature_engineering()
        
        # 4. Ã–zellik hazÄ±rlama
        X, y, feature_columns = self.prepare_features()
        
        # 5. Train/Test split
        self.split_data(X, y)
        
        # 6. Normalizasyon
        self.normalize_data()
        
        # 7. SÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ±
        class_weights = self.calculate_class_weights()
        
        # 8. Kaydet
        self.save_datasets()
        
        print("\n" + "="*80)
        print("âœ… DATASET HAZIRLAMA TAMAMLANDI!")
        print("="*80)
        
        return True


# Test & Run
if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸ“Š DATASET HAZIRLAMA - PHASE 7.B1")
    print("="*80)
    
    # Dataset hazÄ±rlayÄ±cÄ± oluÅŸtur
    preparator = DatasetPreparator('training_dataset.csv')
    
    # Tam iÅŸlemi Ã§alÄ±ÅŸtÄ±r
    success = preparator.process()
    
    if success:
        print("\n" + "="*80)
        print("ğŸ¯ SONRAKÄ° ADIMLAR")
        print("="*80)
        print("\n   1. prepared_data/ klasÃ¶rÃ¼nÃ¼ kontrol edin")
        print("   2. tune_xgboost.py ile XGBoost model eÄŸitimi yapÄ±n")
        print("   3. tune_lightgbm.py ile LightGBM model eÄŸitimi yapÄ±n")
        print("   4. evaluate_models.py ile modelleri karÅŸÄ±laÅŸtÄ±rÄ±n")
        print("\n" + "="*80)
    else:
        print("\nâŒ Dataset hazÄ±rlama baÅŸarÄ±sÄ±z!")
        print("âš ï¸  Ã–nce historical_data_collector.py ve calculate_historical_factors.py Ã§alÄ±ÅŸtÄ±rÄ±n")
