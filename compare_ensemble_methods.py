"""
Phase 7.C2: Ensemble Metod Kar≈üƒ±la≈ütƒ±rmasƒ±
==========================================
Farklƒ± ensemble y√∂ntemlerini (Voting, Averaging, Weighted) kar≈üƒ±la≈ütƒ±rƒ±r.

Ensemble Metodlarƒ±:
1. Hard Voting: √áoƒüunluk oylamasƒ±
2. Soft Voting: Olasƒ±lƒ±k ortalamasƒ±
3. Simple Averaging: E≈üit aƒüƒ±rlƒ±klƒ± ortalama
4. Weighted Averaging: Optimize edilmi≈ü aƒüƒ±rlƒ±klƒ± ortalama
5. Stacking: Meta-learner ile kombinasyon

√ñzellikler:
- T√ºm ensemble metodlarƒ±nƒ± test et
- Performans metriklerini kar≈üƒ±la≈ütƒ±r
- G√∂rselle≈ütirme (confusion matrix, ROC curve, metric comparison)
- En iyi metodu belirle
- Detaylƒ± rapor olu≈ütur

Kullanƒ±m:
    python compare_ensemble_methods.py
    
    # Sadece temel metodlar
    python compare_ensemble_methods.py --basic-only
    
    # T√ºm metodlar + stacking
    python compare_ensemble_methods.py --include-stacking

√áƒ±ktƒ±lar:
- ensemble_comparison/comparison_report.json
- ensemble_comparison/performance_metrics.png
- ensemble_comparison/confusion_matrices.png
- ensemble_comparison/roc_curves.png
- ensemble_comparison/method_recommendation.json
"""

import numpy as np
import pandas as pd
import json
import joblib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import argparse
import warnings
warnings.filterwarnings('ignore')

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# ML
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc,
    roc_auc_score, log_loss
)
from sklearn.preprocessing import label_binarize
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, StratifiedKFold

# ML modelleri
try:
    import xgboost as xgb
    import lightgbm as lgb
    MODELS_AVAILABLE = True
except ImportError:
    MODELS_AVAILABLE = False
    print("‚ö†Ô∏è XGBoost veya LightGBM bulunamadƒ±!")


class EnsembleMethodComparator:
    """Farklƒ± ensemble metodlarƒ±nƒ± kar≈üƒ±la≈ütƒ±rƒ±r."""
    
    def __init__(self, models_dir: str = "models", data_dir: str = "prepared_data",
                 weights_dir: str = "optimized_weights"):
        """
        Args:
            models_dir: Model dosyalarƒ±nƒ±n dizini
            data_dir: Hazƒ±rlanmƒ±≈ü veri dosyalarƒ±nƒ±n dizini
            weights_dir: Optimize edilmi≈ü aƒüƒ±rlƒ±klarƒ±n dizini
        """
        self.models_dir = Path(models_dir)
        self.data_dir = Path(data_dir)
        self.weights_dir = Path(weights_dir)
        self.output_dir = Path("ensemble_comparison")
        self.output_dir.mkdir(exist_ok=True)
        
        self.models = {}
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None
        self.optimized_weights = None
        
        self.results = {}
        
        # Plot settings
        plt.style.use('seaborn-v0_8-darkgrid')
        self.colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']
        
        print("üîß EnsembleMethodComparator ba≈ülatƒ±ldƒ±")
    
    def load_data(self):
        """Hazƒ±rlanmƒ±≈ü veriyi y√ºkle."""
        print("\nüìÇ Veri y√ºkleniyor...")
        
        try:
            self.X_train = np.load(self.data_dir / "X_train.npy")
            self.y_train = np.load(self.data_dir / "y_train.npy")
            self.X_test = np.load(self.data_dir / "X_test.npy")
            self.y_test = np.load(self.data_dir / "y_test.npy")
            
            print(f"   ‚úÖ Train: {self.X_train.shape}")
            print(f"   ‚úÖ Test: {self.X_test.shape}")
            
            return True
            
        except Exception as e:
            print(f"   ‚ùå Veri y√ºkleme hatasƒ±: {e}")
            return False
    
    def load_models(self):
        """T√ºm mevcut modelleri y√ºkle."""
        print("\nü§ñ Modeller y√ºkleniyor...")
        
        model_files = {
            'XGBoost_v1': 'xgb_v1.pkl',
            'XGBoost_v2': 'xgb_v2.pkl',
            'LightGBM_v1': 'lgb_v1.pkl',
            'LightGBM_v2': 'lgb_v2.pkl'
        }
        
        for name, filename in model_files.items():
            filepath = self.models_dir / filename
            if filepath.exists():
                try:
                    self.models[name] = joblib.load(filepath)
                    print(f"   ‚úÖ {name} y√ºklendi")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è {name} y√ºklenemedi: {e}")
        
        print(f"\n   üìä Toplam {len(self.models)} model y√ºklendi")
        return len(self.models) >= 2
    
    def load_optimized_weights(self):
        """Optimize edilmi≈ü aƒüƒ±rlƒ±klarƒ± y√ºkle."""
        print("\n‚öñÔ∏è Optimize edilmi≈ü aƒüƒ±rlƒ±klar y√ºkleniyor...")
        
        weights_file = self.weights_dir / "general_weights.json"
        if weights_file.exists():
            try:
                with open(weights_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.optimized_weights = data.get('weights', None)
                
                if self.optimized_weights:
                    print("   ‚úÖ Aƒüƒ±rlƒ±klar y√ºklendi:")
                    for name, weight in self.optimized_weights.items():
                        print(f"      {name}: {weight:.4f}")
                    return True
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è Aƒüƒ±rlƒ±k y√ºkleme hatasƒ±: {e}")
        
        print("   ‚ö†Ô∏è Optimize edilmi≈ü aƒüƒ±rlƒ±klar bulunamadƒ±")
        print("   ‚ÑπÔ∏è E≈üit aƒüƒ±rlƒ±klar kullanƒ±lacak")
        
        # E≈üit aƒüƒ±rlƒ±klar olu≈ütur
        n_models = len(self.models)
        self.optimized_weights = {name: 1.0/n_models for name in self.models.keys()}
        return False
    
    def method_hard_voting(self) -> Tuple[np.ndarray, Dict]:
        """Hard Voting: √áoƒüunluk oylamasƒ±."""
        print("\nüó≥Ô∏è Hard Voting ensemble...")
        
        # Her modelin tahminlerini al
        predictions = []
        for name, model in self.models.items():
            y_pred = model.predict(self.X_test)
            predictions.append(y_pred)
        
        # √áoƒüunluk oylamasƒ±
        predictions = np.array(predictions)
        y_pred_voting = np.apply_along_axis(
            lambda x: np.bincount(x).argmax(), 
            axis=0, 
            arr=predictions
        )
        
        # Metrikleri hesapla
        metrics = self.calculate_metrics(y_pred_voting)
        
        return y_pred_voting, metrics
    
    def method_soft_voting(self) -> Tuple[np.ndarray, Dict]:
        """Soft Voting: Olasƒ±lƒ±k ortalamasƒ±."""
        print("\nüé≤ Soft Voting ensemble...")
        
        # Her modelin olasƒ±lƒ±k tahminlerini al
        probas = []
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                y_proba = model.predict_proba(self.X_test)
            else:
                # One-hot encoding for models without predict_proba
                y_pred = model.predict(self.X_test)
                n_classes = len(np.unique(self.y_train))
                y_proba = np.zeros((len(y_pred), n_classes))
                y_proba[np.arange(len(y_pred)), y_pred] = 1
            
            probas.append(y_proba)
        
        # Ortalama olasƒ±lƒ±klar
        avg_proba = np.mean(probas, axis=0)
        y_pred_soft = np.argmax(avg_proba, axis=1)
        
        # Metrikleri hesapla
        metrics = self.calculate_metrics(y_pred_soft, y_proba=avg_proba)
        
        return y_pred_soft, metrics
    
    def method_simple_averaging(self) -> Tuple[np.ndarray, Dict]:
        """Simple Averaging: E≈üit aƒüƒ±rlƒ±klƒ± ortalama."""
        print("\nüìä Simple Averaging ensemble...")
        
        # E≈üit aƒüƒ±rlƒ±klar
        n_models = len(self.models)
        equal_weights = {name: 1.0/n_models for name in self.models.keys()}
        
        # Aƒüƒ±rlƒ±klƒ± ortalama
        y_pred, y_proba = self.weighted_average(equal_weights)
        
        # Metrikleri hesapla
        metrics = self.calculate_metrics(y_pred, y_proba=y_proba)
        
        return y_pred, metrics
    
    def method_weighted_averaging(self) -> Tuple[np.ndarray, Dict]:
        """Weighted Averaging: Optimize edilmi≈ü aƒüƒ±rlƒ±klƒ± ortalama."""
        print("\n‚öñÔ∏è Weighted Averaging ensemble...")
        
        # Optimize edilmi≈ü aƒüƒ±rlƒ±klarƒ± kullan
        y_pred, y_proba = self.weighted_average(self.optimized_weights)
        
        # Metrikleri hesapla
        metrics = self.calculate_metrics(y_pred, y_proba=y_proba)
        
        return y_pred, metrics
    
    def method_stacking(self) -> Tuple[np.ndarray, Dict]:
        """Stacking: Meta-learner ile kombinasyon."""
        print("\nüèóÔ∏è Stacking ensemble...")
        
        try:
            # Estimator listesi olu≈ütur
            estimators = [(name, model) for name, model in self.models.items()]
            
            # Meta-learner: Logistic Regression
            meta_learner = LogisticRegression(
                max_iter=1000,
                random_state=42,
                multi_class='multinomial'
            )
            
            # Stacking classifier
            stacking_clf = StackingClassifier(
                estimators=estimators,
                final_estimator=meta_learner,
                cv=5,
                n_jobs=-1
            )
            
            # Eƒüit
            print("   Training stacking classifier...")
            stacking_clf.fit(self.X_train, self.y_train)
            
            # Tahmin
            y_pred = stacking_clf.predict(self.X_test)
            y_proba = stacking_clf.predict_proba(self.X_test)
            
            # Metrikleri hesapla
            metrics = self.calculate_metrics(y_pred, y_proba=y_proba)
            
            return y_pred, metrics
            
        except Exception as e:
            print(f"   ‚ùå Stacking hatasƒ±: {e}")
            return None, None
    
    def weighted_average(self, weights: Dict[str, float]) -> Tuple[np.ndarray, np.ndarray]:
        """Aƒüƒ±rlƒ±klƒ± ortalama hesapla."""
        # Her modelin olasƒ±lƒ±k tahminlerini al
        probas = {}
        for name, model in self.models.items():
            if hasattr(model, 'predict_proba'):
                probas[name] = model.predict_proba(self.X_test)
            else:
                # Fallback
                y_pred = model.predict(self.X_test)
                n_classes = len(np.unique(self.y_train))
                y_proba = np.zeros((len(y_pred), n_classes))
                y_proba[np.arange(len(y_pred)), y_pred] = 1
                probas[name] = y_proba
        
        # Aƒüƒ±rlƒ±klƒ± ortalama
        weighted_proba = np.zeros_like(list(probas.values())[0])
        total_weight = sum(weights.values())
        
        for name, proba in probas.items():
            if name in weights:
                weighted_proba += proba * (weights[name] / total_weight)
        
        # En y√ºksek olasƒ±lƒ±ƒüa sahip sƒ±nƒ±fƒ± se√ß
        y_pred = np.argmax(weighted_proba, axis=1)
        
        return y_pred, weighted_proba
    
    def calculate_metrics(self, y_pred: np.ndarray, y_proba: Optional[np.ndarray] = None) -> Dict:
        """Performans metriklerini hesapla."""
        metrics = {
            'accuracy': float(accuracy_score(self.y_test, y_pred)),
            'precision_macro': float(precision_score(self.y_test, y_pred, average='macro')),
            'recall_macro': float(recall_score(self.y_test, y_pred, average='macro')),
            'f1_macro': float(f1_score(self.y_test, y_pred, average='macro')),
            'precision_weighted': float(precision_score(self.y_test, y_pred, average='weighted')),
            'recall_weighted': float(recall_score(self.y_test, y_pred, average='weighted')),
            'f1_weighted': float(f1_score(self.y_test, y_pred, average='weighted'))
        }
        
        # ROC AUC (sadece proba varsa)
        if y_proba is not None:
            try:
                n_classes = len(np.unique(self.y_train))
                if n_classes == 2:
                    metrics['roc_auc'] = float(roc_auc_score(self.y_test, y_proba[:, 1]))
                else:
                    metrics['roc_auc_ovr'] = float(roc_auc_score(
                        self.y_test, y_proba, multi_class='ovr', average='macro'
                    ))
                    metrics['roc_auc_ovo'] = float(roc_auc_score(
                        self.y_test, y_proba, multi_class='ovo', average='macro'
                    ))
                
                # Log loss
                metrics['log_loss'] = float(log_loss(self.y_test, y_proba))
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è ROC AUC hesaplama hatasƒ±: {e}")
        
        # Confusion matrix
        cm = confusion_matrix(self.y_test, y_pred)
        metrics['confusion_matrix'] = cm.tolist()
        
        # Per-class metrics
        class_report = classification_report(self.y_test, y_pred, output_dict=True)
        metrics['per_class'] = class_report
        
        return metrics
    
    def compare_all_methods(self, include_stacking: bool = False):
        """T√ºm metodlarƒ± kar≈üƒ±la≈ütƒ±r."""
        print("\n" + "="*80)
        print("üîç ENSEMBLE METOD KAR≈ûILA≈ûTIRMASI")
        print("="*80)
        
        methods = {
            'Hard Voting': self.method_hard_voting,
            'Soft Voting': self.method_soft_voting,
            'Simple Averaging': self.method_simple_averaging,
            'Weighted Averaging': self.method_weighted_averaging
        }
        
        if include_stacking:
            methods['Stacking'] = self.method_stacking
        
        # Her metodu test et
        for method_name, method_func in methods.items():
            print(f"\n{'='*60}")
            print(f"üìç {method_name}")
            print(f"{'='*60}")
            
            y_pred, metrics = method_func()
            
            if metrics is not None:
                self.results[method_name] = {
                    'predictions': y_pred,
                    'metrics': metrics
                }
                
                # Metrikleri yazdƒ±r
                print(f"\n   üìä Performans Metrikleri:")
                print(f"      Accuracy:    {metrics['accuracy']:.4f}")
                print(f"      Precision:   {metrics['precision_macro']:.4f}")
                print(f"      Recall:      {metrics['recall_macro']:.4f}")
                print(f"      F1-Score:    {metrics['f1_macro']:.4f}")
                if 'roc_auc_ovr' in metrics:
                    print(f"      ROC AUC:     {metrics['roc_auc_ovr']:.4f}")
                if 'log_loss' in metrics:
                    print(f"      Log Loss:    {metrics['log_loss']:.4f}")
    
    def plot_performance_comparison(self):
        """Performans kar≈üƒ±la≈ütƒ±rma grafiƒüi."""
        print("\nüìä Performans kar≈üƒ±la≈ütƒ±rma grafiƒüi olu≈üturuluyor...")
        
        # Metrikleri topla
        methods = list(self.results.keys())
        metrics_names = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Ensemble Metodlarƒ± - Performans Kar≈üƒ±la≈ütƒ±rmasƒ±', 
                     fontsize=16, fontweight='bold')
        
        for idx, metric in enumerate(metrics_names):
            ax = axes[idx // 2, idx % 2]
            
            values = [self.results[m]['metrics'][metric] for m in methods]
            
            bars = ax.bar(methods, values, color=self.colors[:len(methods)], 
                         alpha=0.7, edgecolor='black')
            
            # Deƒüerleri g√∂ster
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.4f}',
                       ha='center', va='bottom', fontweight='bold')
            
            ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)
            ax.set_ylim([0, 1.1])
            ax.grid(axis='y', alpha=0.3)
            ax.set_xticklabels(methods, rotation=15, ha='right')
        
        plt.tight_layout()
        
        # Kaydet
        filepath = self.output_dir / 'performance_metrics.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        print(f"   ‚úÖ Grafik kaydedildi: {filepath}")
        plt.close()
    
    def plot_confusion_matrices(self):
        """Confusion matrix kar≈üƒ±la≈ütƒ±rmasƒ±."""
        print("\nüéØ Confusion matrix kar≈üƒ±la≈ütƒ±rmasƒ± olu≈üturuluyor...")
        
        n_methods = len(self.results)
        fig, axes = plt.subplots(1, n_methods, figsize=(5*n_methods, 4))
        
        if n_methods == 1:
            axes = [axes]
        
        fig.suptitle('Ensemble Metodlarƒ± - Confusion Matrix Kar≈üƒ±la≈ütƒ±rmasƒ±', 
                     fontsize=16, fontweight='bold')
        
        for idx, (method_name, result) in enumerate(self.results.items()):
            cm = np.array(result['metrics']['confusion_matrix'])
            
            # Normalize
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            
            # Plot
            ax = axes[idx]
            sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                       ax=ax, cbar=True, square=True)
            ax.set_title(method_name, fontsize=12, fontweight='bold')
            ax.set_ylabel('Ger√ßek', fontsize=10)
            ax.set_xlabel('Tahmin', fontsize=10)
        
        plt.tight_layout()
        
        # Kaydet
        filepath = self.output_dir / 'confusion_matrices.png'
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        print(f"   ‚úÖ Grafik kaydedildi: {filepath}")
        plt.close()
    
    def determine_best_method(self) -> Dict:
        """En iyi metodu belirle."""
        print("\nüèÜ En iyi metod belirleniyor...")
        
        # Metriklere g√∂re sƒ±ralama
        rankings = {
            'accuracy': {},
            'f1_macro': {},
            'precision_macro': {},
            'recall_macro': {}
        }
        
        for metric in rankings.keys():
            for method_name, result in self.results.items():
                score = result['metrics'][metric]
                rankings[metric][method_name] = score
        
        # Her metrik i√ßin en iyi metodu bul
        best_by_metric = {}
        for metric, scores in rankings.items():
            best_method = max(scores.items(), key=lambda x: x[1])
            best_by_metric[metric] = {
                'method': best_method[0],
                'score': float(best_method[1])
            }
        
        # Genel en iyi (F1-score bazlƒ±)
        best_overall = max(
            self.results.items(),
            key=lambda x: x[1]['metrics']['f1_macro']
        )
        
        recommendation = {
            'best_overall': {
                'method': best_overall[0],
                'f1_macro': float(best_overall[1]['metrics']['f1_macro']),
                'accuracy': float(best_overall[1]['metrics']['accuracy']),
                'all_metrics': best_overall[1]['metrics']
            },
            'best_by_metric': best_by_metric,
            'all_rankings': {
                metric: sorted(scores.items(), key=lambda x: x[1], reverse=True)
                for metric, scores in rankings.items()
            }
        }
        
        print(f"\n   ü•á EN ƒ∞Yƒ∞ METOD: {best_overall[0]}")
        print(f"      F1-Score: {best_overall[1]['metrics']['f1_macro']:.4f}")
        print(f"      Accuracy: {best_overall[1]['metrics']['accuracy']:.4f}")
        
        print(f"\n   üìä Metrik Bazlƒ± En ƒ∞yiler:")
        for metric, info in best_by_metric.items():
            print(f"      {metric:20s}: {info['method']:20s} ({info['score']:.4f})")
        
        return recommendation
    
    def save_results(self, recommendation: Dict):
        """Sonu√ßlarƒ± kaydet."""
        print("\nüíæ Sonu√ßlar kaydediliyor...")
        
        # Comparison report
        comparison_report = {
            'timestamp': datetime.now().isoformat(),
            'n_models': len(self.models),
            'model_names': list(self.models.keys()),
            'methods_compared': list(self.results.keys()),
            'results': {
                method: {
                    'metrics': result['metrics']
                }
                for method, result in self.results.items()
            },
            'recommendation': recommendation
        }
        
        # JSON kaydet
        filepath = self.output_dir / 'comparison_report.json'
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(comparison_report, f, indent=2, ensure_ascii=False)
        print(f"   ‚úÖ Kar≈üƒ±la≈ütƒ±rma raporu: {filepath}")
        
        # Recommendation kaydet
        filepath = self.output_dir / 'method_recommendation.json'
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(recommendation, f, indent=2, ensure_ascii=False)
        print(f"   ‚úÖ Metod √∂nerisi: {filepath}")
    
    def run(self, include_stacking: bool = False):
        """Kar≈üƒ±la≈ütƒ±rmayƒ± √ßalƒ±≈ütƒ±r."""
        print("\n" + "="*80)
        print("üéØ ENSEMBLE METOD KAR≈ûILA≈ûTIRMASI")
        print("="*80)
        
        # Veri ve modelleri y√ºkle
        if not self.load_data():
            return
        
        if not self.load_models():
            print("‚ùå Yeterli model bulunamadƒ±!")
            return
        
        # Aƒüƒ±rlƒ±klarƒ± y√ºkle
        self.load_optimized_weights()
        
        # T√ºm metodlarƒ± kar≈üƒ±la≈ütƒ±r
        self.compare_all_methods(include_stacking=include_stacking)
        
        if not self.results:
            print("‚ùå Sonu√ß bulunamadƒ±!")
            return
        
        # G√∂rselle≈ütirmeler
        self.plot_performance_comparison()
        self.plot_confusion_matrices()
        
        # En iyi metodu belirle
        recommendation = self.determine_best_method()
        
        # Sonu√ßlarƒ± kaydet
        self.save_results(recommendation)
        
        print("\n" + "="*80)
        print("‚úÖ KAR≈ûILA≈ûTIRMA TAMAMLANDI!")
        print("="*80)
        print(f"\nüìÅ √áƒ±ktƒ±lar: {self.output_dir}")
        print(f"   ‚Ä¢ comparison_report.json")
        print(f"   ‚Ä¢ method_recommendation.json")
        print(f"   ‚Ä¢ performance_metrics.png")
        print(f"   ‚Ä¢ confusion_matrices.png")
        
        return recommendation


def main():
    """Ana fonksiyon."""
    parser = argparse.ArgumentParser(
        description='Ensemble metodlarƒ±nƒ± kar≈üƒ±la≈ütƒ±r'
    )
    parser.add_argument(
        '--include-stacking',
        action='store_true',
        help='Stacking metodunu da dahil et (daha yava≈ü)'
    )
    parser.add_argument(
        '--basic-only',
        action='store_true',
        help='Sadece temel metodlarƒ± test et'
    )
    parser.add_argument(
        '--models-dir',
        type=str,
        default='models',
        help='Model dizini (default: models)'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        default='prepared_data',
        help='Veri dizini (default: prepared_data)'
    )
    parser.add_argument(
        '--weights-dir',
        type=str,
        default='optimized_weights',
        help='Aƒüƒ±rlƒ±k dizini (default: optimized_weights)'
    )
    
    args = parser.parse_args()
    
    # Comparator olu≈ütur
    comparator = EnsembleMethodComparator(
        models_dir=args.models_dir,
        data_dir=args.data_dir,
        weights_dir=args.weights_dir
    )
    
    # √áalƒ±≈ütƒ±r
    include_stacking = args.include_stacking and not args.basic_only
    comparator.run(include_stacking=include_stacking)


if __name__ == "__main__":
    main()
